{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralpheeet/A1-Introduccion-a-Bases-de-Datos-Santander2021/blob/main/Bayesiano_ingenuo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un ejemplo de clasificación en el área médica"
      ],
      "metadata": {
        "id": "n-vkKZyhG4wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con (estimador máximo-verosímil) EMV"
      ],
      "metadata": {
        "id": "mEv6cRA9f0YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Datos de entrenamiento\n",
        "data = {\n",
        "    'Temperatura': [36.5, 37.2, 36.8, 37.5, 36.9, 37.0, 35.3, 37.1, 34.9, 38.1, 36.7, 36.5, 37.22],\n",
        "    'Presion_Arterial': [120, 140, 130, 150, 125, 145, 129, 141, 153, 151, 125, 158, 160],\n",
        "    'Comorbilidad': ['Obesidad', 'Obesidad', 'Ansiedad', 'Asma', 'Asma', 'Arritmia', 'Ansiedad', 'Obesidad',\n",
        "                     'Ansiedad', 'Obesidad', 'Ansiedad', 'Asma', 'Arritmia'],\n",
        "    'Clasificacion': ['S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'E']\n",
        "}\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "label_encoder_comorbilidad = LabelEncoder()\n",
        "df['Comorbilidad'] = label_encoder_comorbilidad.fit_transform(df['Comorbilidad'])\n",
        "label_encoder_clasificacion = LabelEncoder()\n",
        "df['Clasificacion'] = label_encoder_clasificacion.fit_transform(df['Clasificacion'])\n",
        "\n",
        "# Dividir las características (X) y el objetivo (y)\n",
        "X = df[['Temperatura', 'Presion_Arterial', 'Comorbilidad']].values\n",
        "y = df['Clasificacion'].values\n",
        "\n",
        "# Visualizar estadísticas\n",
        "print(\"Estadísticas de las características por clase:\")\n",
        "for c in nb_emv.classes:\n",
        "    print(f\"\\nClase {label_encoder_clasificacion.inverse_transform([c])[0]}:\")\n",
        "    print(f\"Media (Temperatura, Presión, Comorbilidad): {nb_emv.feature_stats[c]['mean']}\")\n",
        "    print(f\"Varianza (Temperatura, Presión, Comorbilidad): {nb_emv.feature_stats[c]['var']}\")\n",
        "\n",
        "print( )\n",
        "# Implementación personalizada de Naive Bayes con EMV\n",
        "class NaiveBayesEMV:\n",
        "    def fit(self, X, y):\n",
        "        self.classes, counts = np.unique(y, return_counts=True)\n",
        "        self.priors = counts / len(y)  # P(y)\n",
        "        self.feature_stats = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.feature_stats[c] = {\n",
        "                'mean': X_c.mean(axis=0),\n",
        "                'var': X_c.var(axis=0),  # Varianza no ajustada\n",
        "            }\n",
        "\n",
        "    def _gaussian_prob(self, x, mean, var):\n",
        "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var)\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        return coeff * exponent\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probabilities = []\n",
        "        for x in X:\n",
        "            class_probs = []\n",
        "            for c in self.classes:\n",
        "                prior = self.priors[c]\n",
        "                likelihoods = np.prod(\n",
        "                    self._gaussian_prob(x, self.feature_stats[c]['mean'], self.feature_stats[c]['var'])\n",
        "                )\n",
        "                class_probs.append(prior * likelihoods)\n",
        "            print(f\"Probabilidades antes de normalizar para {x}: {class_probs}\")\n",
        "            total = np.sum(class_probs)\n",
        "            normalized_probs = [p / total for p in class_probs]  # Normalizar\n",
        "            probabilities.append(normalized_probs)\n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.argmax(probas, axis=1)\n",
        "\n",
        "# Entrenar el modelo personalizado con EMV\n",
        "nb_emv = NaiveBayesEMV()\n",
        "nb_emv.fit(X, y)\n",
        "\n",
        "# Nuevos datos para predecir\n",
        "nuevos_datos = np.array([\n",
        "    [35.9, 143, label_encoder_comorbilidad.transform(['Obesidad'])[0]],\n",
        "    [36.0, 140, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [37.2, 125, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [36.4, 120, label_encoder_comorbilidad.transform(['Arritmia'])[0]],\n",
        "    [36.8, 162, label_encoder_comorbilidad.transform(['Obesidad'])[0]]\n",
        "])\n",
        "\n",
        "# Calcular probabilidades y predicciones\n",
        "probas = nb_emv.predict_proba(nuevos_datos)\n",
        "predicciones_emv = nb_emv.predict(nuevos_datos)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for i, (pred_num, prob) in enumerate(zip(predicciones_emv, probas)):\n",
        "    pred_clase = label_encoder_clasificacion.inverse_transform([pred_num])[0]\n",
        "    print(f\"Paciente {i+1}: Clase Predicha - {pred_clase} (Numérica: {pred_num})\")\n",
        "    print(f\"\\tProbabilidad de Enfermo (E): {prob[0]:.8f}\")\n",
        "    print(f\"\\tProbabilidad de Sano (S): {prob[1]:.8f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymxj5ETWh9K",
        "outputId": "5039debd-93de-41e9-9b5e-2b59b5c0d74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estadísticas de las características por clase:\n",
            "\n",
            "Clase E:\n",
            "Media (Temperatura, Presión, Comorbilidad): [ 37.23142857 149.28571429   2.14285714]\n",
            "Varianza (Temperatura, Presión, Comorbilidad): [ 0.20478367 52.48979592  0.69387755]\n",
            "\n",
            "Clase S:\n",
            "Media (Temperatura, Presión, Comorbilidad): [ 36.18333333 130.33333333   0.83333333]\n",
            "Varianza (Temperatura, Presión, Comorbilidad): [  0.61472222 113.22222222   1.47222222]\n",
            "\n",
            "Probabilidades antes de normalizar para [ 35.9 143.    3. ]: [6.675127098620324e-05, 0.0002711199556535683]\n",
            "Probabilidades antes de normalizar para [ 36. 140.   2.]: [0.0001338128526015201, 0.0011743530406326387]\n",
            "Probabilidades antes de normalizar para [ 37.2 125.    2. ]: [4.468613411430369e-05, 0.0006937719377304019]\n",
            "Probabilidades antes de normalizar para [ 36.4 120.    1. ]: [2.5570626909479106e-07, 0.0017225768150556794]\n",
            "Probabilidades antes de normalizar para [ 36.8 162.    3. ]: [0.0010035069645186955, 5.148602687223667e-06]\n",
            "Probabilidades antes de normalizar para [ 35.9 143.    3. ]: [6.675127098620324e-05, 0.0002711199556535683]\n",
            "Probabilidades antes de normalizar para [ 36. 140.   2.]: [0.0001338128526015201, 0.0011743530406326387]\n",
            "Probabilidades antes de normalizar para [ 37.2 125.    2. ]: [4.468613411430369e-05, 0.0006937719377304019]\n",
            "Probabilidades antes de normalizar para [ 36.4 120.    1. ]: [2.5570626909479106e-07, 0.0017225768150556794]\n",
            "Probabilidades antes de normalizar para [ 36.8 162.    3. ]: [0.0010035069645186955, 5.148602687223667e-06]\n",
            "Paciente 1: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.19756424\n",
            "\tProbabilidad de Sano (S): 0.80243576\n",
            "Paciente 2: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.10229043\n",
            "\tProbabilidad de Sano (S): 0.89770957\n",
            "Paciente 3: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.06051276\n",
            "\tProbabilidad de Sano (S): 0.93948724\n",
            "Paciente 4: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.00014842\n",
            "\tProbabilidad de Sano (S): 0.99985158\n",
            "Paciente 5: Clase Predicha - E (Numérica: 0)\n",
            "\tProbabilidad de Enfermo (E): 0.99489558\n",
            "\tProbabilidad de Sano (S): 0.00510442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplificando implementar EMV con scikit-learn"
      ],
      "metadata": {
        "id": "P3m58ehMfrbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Datos de entrenamiento\n",
        "# Convertir los datos manualmente en un DataFrame\n",
        "data = {\n",
        "    'Temperatura': [36.5, 37.2, 36.8, 37.5, 36.9, 37.0, 35.3, 37.1, 34.9, 38.1, 36.7, 36.5, 37.22],\n",
        "    'Presion_Arterial': [120, 140, 130, 150, 125, 145, 129, 141, 153, 151, 125, 158, 160],\n",
        "    'Comorbilidad': ['Obesidad', 'Obesidad', 'Ansiedad', 'Asma', 'Asma', 'Arritmia', 'Ansiedad', 'Obesidad',\n",
        "                     'Ansiedad', 'Obesidad', 'Ansiedad', 'Asma', 'Arritmia'],\n",
        "    'Clasificacion': ['S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'E']\n",
        "}\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "label_encoder_comorbilidad = LabelEncoder()\n",
        "df['Comorbilidad'] = label_encoder_comorbilidad.fit_transform(df['Comorbilidad'])\n",
        "label_encoder_clasificacion = LabelEncoder()\n",
        "df['Clasificacion'] = label_encoder_clasificacion.fit_transform(df['Clasificacion'])\n",
        "\n",
        "# Dividir las características (X) y el objetivo (y)\n",
        "X = df[['Temperatura', 'Presion_Arterial', 'Comorbilidad']]\n",
        "y = df['Clasificacion']\n",
        "\n",
        "# Entrenar el clasificador bayesiano ingenuo\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X, y)\n",
        "\n",
        "# Nuevos datos para predecir\n",
        "nuevos_datos = [\n",
        "    [35.9, 143, label_encoder_comorbilidad.transform(['Obesidad'])[0]],\n",
        "    [36.0, 140, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [37.2, 125, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [36.4, 120, label_encoder_comorbilidad.transform(['Arritmia'])[0]],\n",
        "    [36.8, 162, label_encoder_comorbilidad.transform(['Obesidad'])[0]]\n",
        "]\n",
        "\n",
        "# Realizar las predicciones\n",
        "predicciones = gnb.predict(nuevos_datos)\n",
        "predicciones_clases = label_encoder_clasificacion.inverse_transform(predicciones)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for i, pred in enumerate(predicciones_clases):\n",
        "    print(f\"Paciente {i+1}: Clase Predicha - {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoxPJIKu4SFA",
        "outputId": "a77c8bef-c793-4d05-f48c-a06f39759bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paciente 1: Clase Predicha - S\n",
            "Paciente 2: Clase Predicha - S\n",
            "Paciente 3: Clase Predicha - S\n",
            "Paciente 4: Clase Predicha - S\n",
            "Paciente 5: Clase Predicha - E\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con estimación máxima a posteriori (MAP)"
      ],
      "metadata": {
        "id": "ukcVhmtrfwyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Datos de entrenamiento\n",
        "# Convertir los datos manualmente en un DataFrame\n",
        "data = {\n",
        "    'Temperatura': [36.5, 37.2, 36.8, 37.5, 36.9, 37.0, 35.3, 37.1, 34.9, 38.1, 36.7, 36.5, 37.22],\n",
        "    'Presion_Arterial': [120, 140, 130, 150, 125, 145, 129, 141, 153, 151, 125, 158, 160],\n",
        "    'Comorbilidad': ['Obesidad', 'Obesidad', 'Ansiedad', 'Asma', 'Asma', 'Arritmia', 'Ansiedad', 'Obesidad',\n",
        "                     'Ansiedad', 'Obesidad', 'Ansiedad', 'Asma', 'Arritmia'],\n",
        "    'Clasificacion': ['S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'E']\n",
        "}\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "label_encoder_comorbilidad = LabelEncoder()\n",
        "df['Comorbilidad'] = label_encoder_comorbilidad.fit_transform(df['Comorbilidad'])\n",
        "label_encoder_clasificacion = LabelEncoder()\n",
        "df['Clasificacion'] = label_encoder_clasificacion.fit_transform(df['Clasificacion'])\n",
        "\n",
        "# Dividir las características (X) y el objetivo (y)\n",
        "X = df[['Temperatura', 'Presion_Arterial', 'Comorbilidad']]\n",
        "y = df['Clasificacion']\n",
        "\n",
        "# Implementación personalizada para MAP\n",
        "class NaiveBayesMAP:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes, counts = np.unique(y, return_counts=True)\n",
        "        self.priors = counts / len(y)\n",
        "        self.feature_stats = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.feature_stats[c] = {\n",
        "                'mean': X_c.mean(axis=0),\n",
        "                'var': X_c.var(axis=0) + self.alpha,\n",
        "            }\n",
        "\n",
        "    def _gaussian_prob(self, x, mean, var):\n",
        "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var)\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        return coeff * exponent\n",
        "\n",
        "    def predict(self, X):\n",
        "        posteriors = []\n",
        "        for x in X:\n",
        "            class_posteriors = []\n",
        "            for c in self.classes:\n",
        "                prior = np.log(self.priors[c])\n",
        "                likelihoods = np.sum(\n",
        "                    np.log(self._gaussian_prob(x, self.feature_stats[c]['mean'], self.feature_stats[c]['var']))\n",
        "                )\n",
        "                class_posteriors.append(prior + likelihoods)\n",
        "            posteriors.append(self.classes[np.argmax(class_posteriors)])\n",
        "        return np.array(posteriors)\n",
        "\n",
        "# Entrenar el modelo personalizado con MAP\n",
        "nb_map = NaiveBayesMAP(alpha=1.0)\n",
        "X_np = X.values\n",
        "y_np = y.values\n",
        "nb_map.fit(X_np, y_np)\n",
        "\n",
        "# Nuevos datos para predecir\n",
        "nuevos_datos = np.array([\n",
        "    [35.9, 143, label_encoder_comorbilidad.transform(['Obesidad'])[0]],\n",
        "    [36.0, 140, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [37.2, 125, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [36.4, 120, label_encoder_comorbilidad.transform(['Arritmia'])[0]],\n",
        "    [36.8, 162, label_encoder_comorbilidad.transform(['Obesidad'])[0]]\n",
        "])\n",
        "\n",
        "# Realizar las predicciones\n",
        "predicciones_map = nb_map.predict(nuevos_datos)\n",
        "predicciones_clases_map = label_encoder_clasificacion.inverse_transform(predicciones_map)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for i, pred in enumerate(predicciones_clases_map):\n",
        "    print(f\"Paciente {i+1}: Clase Predicha (MAP) - {pred}\")\n",
        "    #print(f\"\\tProbabilidad de Sano (S): {proba[0]:.8f}\")\n",
        "    #print(f\"\\tProbabilidad de Enfermo (E): {proba[1]:.8f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sdCiCwQ97vN",
        "outputId": "2ce33ba1-bfa9-433b-8b45-ac240fe6978c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paciente 1: Clase Predicha (MAP) - E\n",
            "Paciente 2: Clase Predicha (MAP) - E\n",
            "Paciente 3: Clase Predicha (MAP) - S\n",
            "Paciente 4: Clase Predicha (MAP) - S\n",
            "Paciente 5: Clase Predicha (MAP) - E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Datos de entrenamiento\n",
        "data = {\n",
        "    'Temperatura': [36.5, 37.2, 36.8, 37.5, 36.9, 37.0, 35.3, 37.1, 34.9, 38.1, 36.7, 36.5, 37.22],\n",
        "    'Presion_Arterial': [120, 140, 130, 150, 125, 145, 129, 141, 153, 151, 125, 158, 160],\n",
        "    'Comorbilidad': ['Obesidad', 'Obesidad', 'Ansiedad', 'Asma', 'Asma', 'Arritmia', 'Ansiedad', 'Obesidad',\n",
        "                     'Ansiedad', 'Obesidad', 'Ansiedad', 'Asma', 'Arritmia'],\n",
        "    'Clasificacion': ['S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'S', 'E', 'E']\n",
        "}\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "label_encoder_comorbilidad = LabelEncoder()\n",
        "df['Comorbilidad'] = label_encoder_comorbilidad.fit_transform(df['Comorbilidad'])\n",
        "label_encoder_clasificacion = LabelEncoder()\n",
        "df['Clasificacion'] = label_encoder_clasificacion.fit_transform(df['Clasificacion'])\n",
        "\n",
        "# Dividir las características (X) y el objetivo (y)\n",
        "X = df[['Temperatura', 'Presion_Arterial', 'Comorbilidad']].values\n",
        "y = df['Clasificacion'].values\n",
        "\n",
        "# Implementación personalizada para MAP\n",
        "class NaiveBayesMAP:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes, counts = np.unique(y, return_counts=True)\n",
        "        self.priors = counts / len(y)\n",
        "        self.feature_stats = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.feature_stats[c] = {\n",
        "                'mean': X_c.mean(axis=0),\n",
        "                'var': X_c.var(axis=0) + self.alpha,\n",
        "            }\n",
        "\n",
        "    def _gaussian_prob(self, x, mean, var):\n",
        "        coeff = 1.0 / np.sqrt(2.0 * np.pi * var)\n",
        "        exponent = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        return coeff * exponent\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probabilities = []\n",
        "        for x in X:\n",
        "            class_probs = []\n",
        "            for c in self.classes:\n",
        "                prior = np.log(self.priors[c])\n",
        "                likelihoods = np.sum(\n",
        "                    np.log(self._gaussian_prob(x, self.feature_stats[c]['mean'], self.feature_stats[c]['var']))\n",
        "                )\n",
        "                class_probs.append(np.exp(prior + likelihoods))  # Convertir de log-probabilidad a probabilidad\n",
        "            print(f\"Probabilidades antes de normalizar para {x}: {class_probs}\")\n",
        "            total = np.sum(class_probs)\n",
        "            normalized_probs = [p / total for p in class_probs]  # Normalizar\n",
        "            probabilities.append(normalized_probs)\n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.argmax(probas, axis=1)\n",
        "\n",
        "# Entrenar el modelo personalizado con MAP\n",
        "nb_map = NaiveBayesMAP(alpha=1.0)\n",
        "nb_map.fit(X, y)\n",
        "\n",
        "# Nuevos datos para predecir\n",
        "nuevos_datos = np.array([\n",
        "    [35.9, 143, label_encoder_comorbilidad.transform(['Obesidad'])[0]],\n",
        "    [36.0, 140, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [37.2, 125, label_encoder_comorbilidad.transform(['Asma'])[0]],\n",
        "    [36.4, 120, label_encoder_comorbilidad.transform(['Arritmia'])[0]],\n",
        "    [36.8, 162, label_encoder_comorbilidad.transform(['Obesidad'])[0]]\n",
        "])\n",
        "\n",
        "# Calcular probabilidades y predicciones\n",
        "probas = nb_map.predict_proba(nuevos_datos)\n",
        "predicciones_map = nb_map.predict(nuevos_datos)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for i, (pred_num, prob) in enumerate(zip(predicciones_map, probas)):\n",
        "    pred_clase = label_encoder_clasificacion.inverse_transform([pred_num])[0]\n",
        "    print(f\"Paciente {i+1}: Clase Predicha - {pred_clase} (Numérica: {pred_num})\")\n",
        "    print(f\"\\tProbabilidad de Enfermo (E): {prob[0]:.8f}\")\n",
        "    print(f\"\\tProbabilidad de Sano (S): {prob[1]:.8f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh-AUM0fVl7B",
        "outputId": "a0a7ee8f-4092-4f6c-9964-99394d7793c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades antes de normalizar para [ 35.9 143.    3. ]: [0.0008725019478727966, 0.0002566365904346549]\n",
            "Probabilidades antes de normalizar para [ 36. 140.   2.]: [0.0007742575573920613, 0.000685094407657408]\n",
            "Probabilidades antes de normalizar para [ 37.2 125.    2. ]: [1.311364363846587e-05, 0.0006681007205018544]\n",
            "Probabilidades antes de normalizar para [ 36.4 120.    1. ]: [5.509246339243141e-07, 0.0008427961684071084]\n",
            "Probabilidades antes de normalizar para [ 36.8 162.    3. ]: [0.0005381146778204253, 5.856247236009642e-06]\n",
            "Probabilidades antes de normalizar para [ 35.9 143.    3. ]: [0.0008725019478727966, 0.0002566365904346549]\n",
            "Probabilidades antes de normalizar para [ 36. 140.   2.]: [0.0007742575573920613, 0.000685094407657408]\n",
            "Probabilidades antes de normalizar para [ 37.2 125.    2. ]: [1.311364363846587e-05, 0.0006681007205018544]\n",
            "Probabilidades antes de normalizar para [ 36.4 120.    1. ]: [5.509246339243141e-07, 0.0008427961684071084]\n",
            "Probabilidades antes de normalizar para [ 36.8 162.    3. ]: [0.0005381146778204253, 5.856247236009642e-06]\n",
            "Paciente 1: Clase Predicha - E (Numérica: 0)\n",
            "\tProbabilidad de Enfermo (E): 0.77271470\n",
            "\tProbabilidad de Sano (S): 0.22728530\n",
            "Paciente 2: Clase Predicha - E (Numérica: 0)\n",
            "\tProbabilidad de Enfermo (E): 0.53054888\n",
            "\tProbabilidad de Sano (S): 0.46945112\n",
            "Paciente 3: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.01925039\n",
            "\tProbabilidad de Sano (S): 0.98074961\n",
            "Paciente 4: Clase Predicha - S (Numérica: 1)\n",
            "\tProbabilidad de Enfermo (E): 0.00065326\n",
            "\tProbabilidad de Sano (S): 0.99934674\n",
            "Paciente 5: Clase Predicha - E (Numérica: 0)\n",
            "\tProbabilidad de Enfermo (E): 0.98923426\n",
            "\tProbabilidad de Sano (S): 0.01076574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un ejemplo de clasificación de spam de un repositorio"
      ],
      "metadata": {
        "id": "XbRcOaIsg1Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se clona el repositorio"
      ],
      "metadata": {
        "id": "tBLO164N6BRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOPu8f064u6q",
        "outputId": "e49599e4-7e00-4ccd-ecc7-1bcea5fc233a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spam-Detector'...\n",
            "remote: Enumerating objects: 2317, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 2317 (delta 8), reused 4 (delta 1), pack-reused 2283 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2317/2317), 4.90 MiB | 10.12 MiB/s, done.\n",
            "Resolving deltas: 100% (1170/1170), done.\n",
            "/content/Spam-Detector\n",
            " gif   image   README.md  'Spam Detector'\n"
          ]
        }
      ],
      "source": [
        "# Clonar el repositorio\n",
        "!git clone https://github.com/ma-shamshiri/Spam-Detector.git\n",
        "\n",
        "# Cambiar al directorio del repositorio\n",
        "%cd Spam-Detector\n",
        "\n",
        "# Verificar el contenido del repositorio\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones contenidas en spam_detector.py"
      ],
      "metadata": {
        "id": "gvgIzYMa6E8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Establecer las rutas de los archivos de entrenamiento y prueba en Colab\n",
        "train_path = \"/content/Spam-Detector/Spam Detector/train\"\n",
        "test_path = \"/content/Spam-Detector/Spam Detector/test\"\n",
        "\n",
        "# *** Devuelve el número total de correos electrónicos (archivos de entrenamiento) ***\n",
        "def number_of_allEmails():\n",
        "    counter = 0\n",
        "    for directories, subdirectories, files in os.walk(train_path):\n",
        "        for filename in files:\n",
        "            counter += 1\n",
        "    return counter\n",
        "\n",
        "# *** Devuelve el número de correos electrónicos spam (archivos de entrenamiento) ***\n",
        "def number_of_spamEmails():\n",
        "    counter = 0\n",
        "    for directories, subdirectories, files in os.walk(train_path):\n",
        "        for filename in files:\n",
        "            if \"spam\" in filename:\n",
        "                counter += 1\n",
        "    return counter\n",
        "\n",
        "# *** Devuelve el número de correos electrónicos ham (archivos de entrenamiento) ***\n",
        "def number_of_hamEmails():\n",
        "    counter = 0\n",
        "    for directories, subdirectories, files in os.walk(train_path):\n",
        "        for filename in files:\n",
        "            if \"ham\" in filename:\n",
        "                counter += 1\n",
        "    return counter\n",
        "\n",
        "# *** Parsea el texto dado dividiéndolo por caracteres no alfabéticos ***\n",
        "def text_parser(text):\n",
        "    words = re.split(\"[^a-zA-Z]\", text)\n",
        "    lower_words = [word.lower() for word in words if len(word) > 0]\n",
        "    return lower_words\n",
        "\n",
        "# *** Genera las palabras del conjunto de entrenamiento: todas, spam y ham ***\n",
        "def trainWord_generator():\n",
        "    all_words = []\n",
        "    spam_words = []\n",
        "    ham_words = []\n",
        "    for directories, subdirectories, files in os.walk(train_path):\n",
        "        for filename in files:\n",
        "            full_path = os.path.join(directories, filename)\n",
        "            with open(full_path, 'r', encoding='latin-1') as target_file:\n",
        "                data = target_file.read()\n",
        "                words = text_parser(data)\n",
        "                for word in words:\n",
        "                    all_words.append(word)\n",
        "                    if \"ham\" in filename:\n",
        "                        ham_words.append(word)\n",
        "                    elif \"spam\" in filename:\n",
        "                        spam_words.append(word)\n",
        "    return sorted(all_words), sorted(spam_words), sorted(ham_words)\n",
        "\n",
        "# *** Devuelve todas las palabras únicas ***\n",
        "def unique_words(all_trainWords):\n",
        "    return sorted(list(set(all_trainWords)))\n",
        "\n",
        "# *** Calcula la frecuencia de las palabras dadas ***\n",
        "def frequency_calculator(words):\n",
        "    wf = {}\n",
        "    for word in words:\n",
        "        wf[word] = wf.get(word, 0) + 1\n",
        "    return wf\n",
        "\n",
        "# *** Genera la frecuencia de cada palabra en las clases spam y ham ***\n",
        "def bagOfWords_genarator(all_uniqueWords, spam_trainWords, ham_trainWords):\n",
        "    spam_bagOfWords = frequency_calculator(spam_trainWords)\n",
        "    ham_bagOfWords = frequency_calculator(ham_trainWords)\n",
        "    for word in all_uniqueWords:\n",
        "        spam_bagOfWords[word] = spam_bagOfWords.get(word, 0)\n",
        "        ham_bagOfWords[word] = ham_bagOfWords.get(word, 0)\n",
        "    return dict(sorted(spam_bagOfWords.items())), dict(sorted(ham_bagOfWords.items()))\n",
        "\n",
        "# *** Genera Bag of Words suavizados para las clases spam y ham ***\n",
        "def smoothed_bagOfWords(all_uniqueWords, spam_bagOfWords, ham_bagOfWords, delta):\n",
        "    smoothed_spamBOW = {word: spam_bagOfWords[word] + delta for word in all_uniqueWords}\n",
        "    smoothed_hamBOW = {word: ham_bagOfWords[word] + delta for word in all_uniqueWords}\n",
        "    return smoothed_spamBOW, smoothed_hamBOW\n",
        "\n",
        "# *** Calcula la probabilidad de la clase spam: P(spam) ***\n",
        "def spam_probability(nb_of_allEmails, nb_of_spamEmails):\n",
        "    return nb_of_spamEmails / nb_of_allEmails\n",
        "\n",
        "# *** Calcula la probabilidad de la clase ham: P(ham) ***\n",
        "def ham_probability(nb_of_allEmails, nb_of_hamEmails):\n",
        "    return nb_of_hamEmails / nb_of_allEmails\n",
        "\n",
        "# *** Calcula P(Wi|spam) para cada palabra ***\n",
        "def spam_condProbability(all_uniqueWords, spam_bagOfWords, smoothed_spamBOW, delta):\n",
        "    total_wf = sum(spam_bagOfWords.values()) + delta * len(all_uniqueWords)\n",
        "    return {word: smoothed_spamBOW[word] / total_wf for word in smoothed_spamBOW}\n",
        "\n",
        "# *** Calcula P(Wi|ham) para cada palabra ***\n",
        "def ham_condProbability(all_uniqueWords, ham_bagOfWords, smoothed_hamBOW, delta):\n",
        "    total_wf = sum(ham_bagOfWords.values()) + delta * len(all_uniqueWords)\n",
        "    return {word: smoothed_hamBOW[word] / total_wf for word in smoothed_hamBOW}\n",
        "\n",
        "# *** Genera el contenido de model.txt ***\n",
        "def model_output_generator(word_numbers, words, ham_wf, ham_cp, spam_wf, spam_cp):\n",
        "    output = \"\\n\".join([f\"{i+1}  {word}  {ham_wf[word]}  {ham_cp[word]}  {spam_wf[word]}  {spam_cp[word]}\" for i, word in enumerate(words)])\n",
        "    return output\n",
        "\n",
        "def modelFileBuilder(model_output):\n",
        "    # Guarda el archivo model.txt en la carpeta /content/\n",
        "    with open(\"/content/model.txt\", 'w', encoding='utf-8') as model_file:\n",
        "        model_file.write(model_output)\n",
        "\n",
        "# *** Devuelve los nombres de todos los correos electrónicos (archivos de prueba) ***\n",
        "def get_testFileNames():\n",
        "    file_names = []\n",
        "    for directories, subdirectories, files in os.walk(test_path):\n",
        "        for filename in files:\n",
        "            file_names.append(filename)\n",
        "    return file_names\n",
        "\n",
        "# *** Devuelve las etiquetas reales (ham o spam) de cada correo electrónico ***\n",
        "def get_actualLabels():\n",
        "    actual_labels = []\n",
        "    for directories, subdirectories, files in os.walk(test_path):\n",
        "        for filename in files:\n",
        "            if \"ham\" in filename:\n",
        "                actual_labels.append(\"ham\")\n",
        "            else:\n",
        "                actual_labels.append(\"spam\")\n",
        "    return actual_labels\n",
        "\n",
        "# *** Calcula las puntuaciones de ham y spam para cada correo electrónico ***\n",
        "def score_calculator(all_uniqueWords, spam_prob, ham_prob, spam_condProb, ham_condProb, delta):\n",
        "    ham_scores, spam_scores = [], []\n",
        "    predicted_labels, decision_labels = [], []\n",
        "    for directories, subdirectories, files in os.walk(test_path):\n",
        "        for filename in files:\n",
        "            actual_label = \"ham\" if \"ham\" in filename else \"spam\"\n",
        "            full_path = os.path.join(directories, filename)\n",
        "            with open(full_path, encoding=\"latin-1\") as target_file:\n",
        "                email_content = target_file.read()\n",
        "                email_words = text_parser(email_content)\n",
        "                sigma_spamScore = sum(\n",
        "                    np.log(spam_condProb[word]) for word in email_words if word in all_uniqueWords\n",
        "                )\n",
        "                sigma_hamScore = sum(\n",
        "                    np.log(ham_condProb[word]) for word in email_words if word in all_uniqueWords\n",
        "                )\n",
        "                spam_score = np.log(spam_prob) + sigma_spamScore\n",
        "                ham_score = np.log(ham_prob) + sigma_hamScore\n",
        "                spam_scores.append(spam_score)\n",
        "                ham_scores.append(ham_score)\n",
        "                predicted_label = \"spam\" if spam_score > ham_score else \"ham\"\n",
        "                predicted_labels.append(predicted_label)\n",
        "                decision_labels.append(\"right\" if predicted_label == actual_label else \"wrong\")\n",
        "    return ham_scores, spam_scores, predicted_labels, decision_labels\n",
        "\n",
        "# *** Genera el contenido de result.txt ***\n",
        "def result_output_generator(fileNumbers, fileNames, predictedLabels, hamScores, spamScores, actualLabels, decisionLabels):\n",
        "    output = \"\"\n",
        "    for i in range(fileNumbers):\n",
        "        output += f\"{i+1}  {fileNames[i]}  {predictedLabels[i]}  {hamScores[i]}  {spamScores[i]}  {actualLabels[i]}  {decisionLabels[i]}\\n\"\n",
        "    return output\n",
        "\n",
        "# *** Crea el archivo result.txt ***\n",
        "def resultFileBuilder(result_output):\n",
        "    with open(\"/content/result.txt\", 'w', encoding='utf-8') as model_file:\n",
        "        model_file.write(result_output)\n",
        "\n",
        "# *** Calcula métricas de precisión, recall, exactitud y F1-measure para la clase spam ***\n",
        "def get_spamPrecision(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"spam\" and predictedLabels[i] == \"spam\")\n",
        "    fp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"ham\" and predictedLabels[i] == \"spam\")\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "def get_spamRecall(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"spam\" and predictedLabels[i] == \"spam\")\n",
        "    fn = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"spam\" and predictedLabels[i] == \"ham\")\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "def get_spamAccuracy(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp_tn = sum(1 for i in range(fileNumbers) if actualLabels[i] == predictedLabels[i])\n",
        "    return tp_tn / fileNumbers\n",
        "\n",
        "def get_spamFmeasure(spam_precision, spam_recall):\n",
        "    return 2 * ((spam_precision * spam_recall) / (spam_precision + spam_recall))\n",
        "\n",
        "# *** Calcula métricas de precisión, recall, exactitud y F1-measure para la clase ham ***\n",
        "def get_hamPrecision(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"ham\" and predictedLabels[i] == \"ham\")\n",
        "    fp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"spam\" and predictedLabels[i] == \"ham\")\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "def get_hamRecall(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"ham\" and predictedLabels[i] == \"ham\")\n",
        "    fn = sum(1 for i in range(fileNumbers) if actualLabels[i] == \"ham\" and predictedLabels[i] == \"spam\")\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "def get_hamAccuracy(fileNumbers, actualLabels, predictedLabels):\n",
        "    tp_tn = sum(1 for i in range(fileNumbers) if actualLabels[i] == predictedLabels[i])\n",
        "    return tp_tn / fileNumbers\n",
        "\n",
        "def get_hamFmeasure(ham_precision, ham_recall):\n",
        "    return 2 * ((ham_precision * ham_recall) / (ham_precision + ham_recall))\n",
        "\n",
        "# *** Genera la tabla de resultados de evaluación ***\n",
        "def evaluation_result(spam_accuracy, spam_precision, spam_recall, spam_fmeasure, ham_accuracy, ham_precision, ham_recall, ham_fmeasure):\n",
        "    return (\n",
        "        \"################################################################################## \\n\"\n",
        "        \"#                           *** Resultados de Evaluación ***                     # \\n\"\n",
        "        \"#                                                                                # \\n\"\n",
        "        f\"#  Spam: Acc: {spam_accuracy} Prec: {spam_precision} Recall: {spam_recall} F1: {spam_fmeasure}      # \\n\"\n",
        "        f\"#   Ham: Acc: {ham_accuracy} Prec: {ham_precision} Recall: {ham_recall} F1: {ham_fmeasure}         # \\n\"\n",
        "        \"################################################################################## \\n\"\n",
        "    )\n",
        "\n",
        "# *** Genera el archivo evaluation.txt ***\n",
        "def evaluationFileBuilder(evaluation_output):\n",
        "    with open(\"/content/evaluation.txt\", 'w', encoding='utf-8') as model_file:\n",
        "        model_file.write(evaluation_output)"
      ],
      "metadata": {
        "id": "-m3bxrhB8PxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase de entrenamiento de train.py"
      ],
      "metadata": {
        "id": "xqsnQatoBT0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "# *************** FASE DE ENTRENAMIENTO ***************\n",
        "################################################\n",
        "\n",
        "def entrenamiento():\n",
        "    print(\"Inicio de la fase de entrenamiento...\\n\")\n",
        "\n",
        "    # Número total de correos electrónicos\n",
        "    nb_of_allEmails = number_of_allEmails()\n",
        "    print(f\"Número total de correos electrónicos: {nb_of_allEmails}\")\n",
        "\n",
        "    # Número de correos electrónicos spam\n",
        "    nb_of_spamEmails = number_of_spamEmails()\n",
        "    print(f\"Número de correos electrónicos spam: {nb_of_spamEmails}\")\n",
        "\n",
        "    # Número de correos electrónicos ham\n",
        "    nb_of_hamEmails = number_of_hamEmails()\n",
        "    print(f\"Número de correos electrónicos deseados: {nb_of_hamEmails}\\n\")\n",
        "\n",
        "    # Palabras del conjunto de entrenamiento: (all_trainWords, spam_trainWords, ham_trainWords)\n",
        "    all_trainWords, spam_trainWords, ham_trainWords = trainWord_generator()\n",
        "    print(f\"Cantidad de palabras en los correos de entrenamiento: {len(all_trainWords)}\")\n",
        "    print(f\"Cantidad de palabras únicas en correos spam: {len(set(spam_trainWords))}\")\n",
        "    print(f\"Cantidad de palabras únicas en correos deseados: {len(set(ham_trainWords))}\\n\")\n",
        "\n",
        "    # Todas las palabras únicas\n",
        "    all_uniqueWords = unique_words(all_trainWords)\n",
        "    print(f\"Cantidad total de palabras únicas: {len(all_uniqueWords)}\\n\")\n",
        "\n",
        "    # Frecuencia de cada palabra en las clases spam y ham\n",
        "    spam_bagOfWords, ham_bagOfWords = bagOfWords_genarator(all_uniqueWords, spam_trainWords, ham_trainWords)\n",
        "    print(f\"Frecuencia total de palabras en spam: {sum(spam_bagOfWords.values())}\")\n",
        "    print(f\"Frecuencia total de palabras en deseados: {sum(ham_bagOfWords.values())}\\n\")\n",
        "\n",
        "    # Bag of Words suavizados: (smoothed_spamBOW, smoothed_hamBOW)\n",
        "    smoothed_spamBOW, smoothed_hamBOW = smoothed_bagOfWords(all_uniqueWords, spam_bagOfWords, ham_bagOfWords, 0.5)\n",
        "    print(\"Se han aplicado suavizados a las bolsas de palabras.\\n\")\n",
        "\n",
        "    # Probabilidad de la clase spam: P(spam)\n",
        "    spam_prob = spam_probability(nb_of_allEmails, nb_of_spamEmails)\n",
        "    print(f\"Probabilidad de clase spam (P(spam)): {spam_prob:.4f}\")\n",
        "\n",
        "    # Probabilidad de la clase ham: P(ham)\n",
        "    ham_prob = ham_probability(nb_of_allEmails, nb_of_hamEmails)\n",
        "    print(f\"Probabilidad de clase deseado (P(deseado)): {ham_prob:.4f}\\n\")\n",
        "\n",
        "    # P(Wi|spam) para cada palabra\n",
        "    spam_condProb = spam_condProbability(all_uniqueWords, spam_bagOfWords, smoothed_spamBOW, 0.5)\n",
        "    print(f\"Ejemplo de P(Wi|spam) para la palabra '{list(spam_condProb.keys())[0]}': {list(spam_condProb.values())[0]:.6f}\")\n",
        "\n",
        "    # P(Wi|ham) para cada palabra\n",
        "    ham_condProb = ham_condProbability(all_uniqueWords, ham_bagOfWords, smoothed_hamBOW, 0.5)\n",
        "    print(f\"Ejemplo de P(Wi|deseado) para la palabra '{list(ham_condProb.keys())[0]}': {list(ham_condProb.values())[0]:.6f}\\n\")\n",
        "\n",
        "    # Variables de salida (para model.txt)\n",
        "    word_numbers = len(all_uniqueWords)\n",
        "    words = all_uniqueWords\n",
        "    ham_wf = ham_bagOfWords\n",
        "    ham_cp = ham_condProb\n",
        "    spam_wf = spam_bagOfWords\n",
        "    spam_cp = spam_condProb\n",
        "\n",
        "    # Genera el contenido de model.txt\n",
        "    model_output = model_output_generator(word_numbers, words, ham_wf, ham_cp, spam_wf, spam_cp)\n",
        "    print(\"Contenido de model.txt generado.\\n\")\n",
        "\n",
        "    # Crea el archivo model.txt\n",
        "    modelFileBuilder(model_output)\n",
        "    print(\"Archivo model.txt guardado en /content/\\n\")\n",
        "    print(\"Fase de entrenamiento completada.\")"
      ],
      "metadata": {
        "id": "Kr03sPEG-fXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear model.txt\n",
        "entrenamiento()"
      ],
      "metadata": {
        "id": "SsoeVLZABsfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9065ba-34ee-4d3b-f726-a7287796e293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicio de la fase de entrenamiento...\n",
            "\n",
            "Número total de correos electrónicos: 1169\n",
            "Número de correos electrónicos spam: 267\n",
            "Número de correos electrónicos deseados: 902\n",
            "\n",
            "Cantidad de palabras en los correos de entrenamiento: 747421\n",
            "Cantidad de palabras únicas en correos spam: 21271\n",
            "Cantidad de palabras únicas en correos deseados: 18199\n",
            "\n",
            "Cantidad total de palabras únicas: 34022\n",
            "\n",
            "Frecuencia total de palabras en spam: 259004\n",
            "Frecuencia total de palabras en deseados: 488417\n",
            "\n",
            "Se han aplicado suavizados a las bolsas de palabras.\n",
            "\n",
            "Probabilidad de clase spam (P(spam)): 0.2284\n",
            "Probabilidad de clase deseado (P(deseado)): 0.7716\n",
            "\n",
            "Ejemplo de P(Wi|spam) para la palabra 'a': 0.023033\n",
            "Ejemplo de P(Wi|deseado) para la palabra 'a': 0.011526\n",
            "\n",
            "Contenido de model.txt generado.\n",
            "\n",
            "Archivo model.txt guardado en /content/\n",
            "\n",
            "Fase de entrenamiento completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase de prueba de test.py"
      ],
      "metadata": {
        "id": "IUPCsxQxEc3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar tqdm para visualizar el progreso\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluar():\n",
        "    print(\"Inicio de la fase de prueba...\\n\")\n",
        "\n",
        "    # Nombres de todos los correos electrónicos\n",
        "    test_fileNames = get_testFileNames()\n",
        "    print(f\"Número total de correos electrónicos a probar: {len(test_fileNames)}\\n\")\n",
        "\n",
        "    # Estadísticas del conjunto de entrenamiento\n",
        "    nb_of_allEmails = number_of_allEmails()\n",
        "    nb_of_spamEmails = number_of_spamEmails()\n",
        "    nb_of_hamEmails = number_of_hamEmails()\n",
        "    print(f\"Número total de correos en el entrenamiento: {nb_of_allEmails}\")\n",
        "    print(f\"Correos spam en el entrenamiento: {nb_of_spamEmails}\")\n",
        "    print(f\"Correos deseados en el entrenamiento: {nb_of_hamEmails}\\n\")\n",
        "\n",
        "    # Procesamiento del conjunto de entrenamiento\n",
        "    all_trainWords, spam_trainWords, ham_trainWords = trainWord_generator()\n",
        "    all_uniqueWords = unique_words(all_trainWords)\n",
        "    spam_bagOfWords, ham_bagOfWords = bagOfWords_genarator(all_uniqueWords, spam_trainWords, ham_trainWords)\n",
        "    smoothed_spamBOW, smoothed_hamBOW = smoothed_bagOfWords(all_uniqueWords, spam_bagOfWords, ham_bagOfWords, 0.5)\n",
        "    spam_prob = spam_probability(nb_of_allEmails, nb_of_spamEmails)\n",
        "    ham_prob = ham_probability(nb_of_allEmails, nb_of_hamEmails)\n",
        "    spam_condProb = spam_condProbability(all_uniqueWords, spam_bagOfWords, smoothed_spamBOW, 0.5)\n",
        "    ham_condProb = ham_condProbability(all_uniqueWords, ham_bagOfWords, smoothed_hamBOW, 0.5)\n",
        "\n",
        "    print(\"Modelo cargado y listo para clasificar correos electrónicos.\\n\")\n",
        "\n",
        "    # Procesamiento de correos de prueba\n",
        "    print(\"Clasificando correos electrónicos...\")\n",
        "    ham_scores, spam_scores, predicted_labels, decision_labels = [], [], [], []\n",
        "\n",
        "    for filename in tqdm(test_fileNames, desc=\"Procesando correos\"):\n",
        "        actual_label = \"ham\" if \"ham\" in filename else \"spam\"\n",
        "        with open(f\"{test_path}/{filename}\", encoding=\"latin-1\") as target_file:\n",
        "            email_content = target_file.read()\n",
        "            email_words = text_parser(email_content)\n",
        "\n",
        "            sigma_spamScore = sum(\n",
        "                np.log(spam_condProb[word]) for word in email_words if word in all_uniqueWords\n",
        "            )\n",
        "            sigma_hamScore = sum(\n",
        "                np.log(ham_condProb[word]) for word in email_words if word in all_uniqueWords\n",
        "            )\n",
        "\n",
        "            spam_score = np.log(spam_prob) + sigma_spamScore\n",
        "            ham_score = np.log(ham_prob) + sigma_hamScore\n",
        "\n",
        "            spam_scores.append(spam_score)\n",
        "            ham_scores.append(ham_score)\n",
        "\n",
        "            predicted_label = \"spam\" if spam_score > ham_score else \"ham\"\n",
        "            predicted_labels.append(predicted_label)\n",
        "\n",
        "            decision_label = \"right\" if predicted_label == actual_label else \"wrong\"\n",
        "            decision_labels.append(decision_label)\n",
        "\n",
        "    print(\"\\nClasificación completada.\\n\")\n",
        "\n",
        "    # Resultados de evaluación\n",
        "    fileNumbers = len(test_fileNames)\n",
        "    result_output = result_output_generator(\n",
        "        fileNumbers, test_fileNames, predicted_labels, ham_scores, spam_scores,\n",
        "        get_actualLabels(), decision_labels\n",
        "    )\n",
        "    resultFileBuilder(result_output)\n",
        "    print(\"Archivo result.txt generado y guardado en /content/.\\n\")\n",
        "    # Análisis de rendimiento\n",
        "    spam_precision = get_spamPrecision(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    spam_recall = get_spamRecall(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    spam_accuracy = get_spamAccuracy(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    spam_fmeasure = get_spamFmeasure(spam_precision, spam_recall)\n",
        "\n",
        "    ham_precision = get_hamPrecision(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    ham_recall = get_hamRecall(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    ham_accuracy = get_hamAccuracy(fileNumbers, get_actualLabels(), predicted_labels)\n",
        "    ham_fmeasure = get_hamFmeasure(ham_precision, ham_recall)\n",
        "\n",
        "    print(\"Resultados del análisis:\\n\")\n",
        "    print(f\"Spam - Precisión: {spam_precision:.4f}, Recall: {spam_recall:.4f}, Exactitud: {spam_accuracy:.4f}, F1: {spam_fmeasure:.4f}\")\n",
        "    print(f\"Ham - Precisión: {ham_precision:.4f}, Recall: {ham_recall:.4f}, Exactitud: {ham_accuracy:.4f}, F1: {ham_fmeasure:.4f}\\n\")\n",
        "\n",
        "    evaluation_result_output = evaluation_result(\n",
        "        spam_accuracy, spam_precision, spam_recall, spam_fmeasure,\n",
        "        ham_accuracy, ham_precision, ham_recall, ham_fmeasure\n",
        "    )\n",
        "    evaluationFileBuilder(evaluation_result_output)\n",
        "    print(\"Archivo evaluation.txt generado y guardado en /content/.\\n\")\n",
        "\n",
        "    print(\"Fase de prueba completada.\")"
      ],
      "metadata": {
        "id": "J19SCfkS5Xgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama a la fase de prueba\n",
        "evaluar()"
      ],
      "metadata": {
        "id": "W3whJkXL9G1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c696d0-9b08-4307-c8d5-2177b5f8aae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicio de la fase de prueba...\n",
            "\n",
            "Número total de correos electrónicos a probar: 800\n",
            "\n",
            "Número total de correos en el entrenamiento: 1169\n",
            "Correos spam en el entrenamiento: 267\n",
            "Correos deseados en el entrenamiento: 902\n",
            "\n",
            "Modelo cargado y listo para clasificar correos electrónicos.\n",
            "\n",
            "Clasificando correos electrónicos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando correos: 100%|██████████| 800/800 [16:14<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clasificación completada.\n",
            "\n",
            "Archivo result.txt generado y guardado en /content/.\n",
            "\n",
            "Resultados del análisis:\n",
            "\n",
            "Spam - Precisión: 0.9851, Recall: 0.8250, Exactitud: 0.9062, F1: 0.8980\n",
            "Ham - Precisión: 0.8495, Recall: 0.9875, Exactitud: 0.9062, F1: 0.9133\n",
            "\n",
            "Archivo evaluation.txt generado y guardado en /content/.\n",
            "\n",
            "Fase de prueba completada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bibliografía\n",
        "\n",
        "Shamshiri, M. (2020). Spam Detector [Código fuente]. GitHub. Recuperado de https://github.com/ma-shamshiri/Spam-Detector\n"
      ],
      "metadata": {
        "id": "IGYtbmY6zPMu"
      }
    }
  ]
}